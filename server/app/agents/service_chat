import asyncio
import pathlib

from langchain_core.messages import AIMessage, ToolMessage, SystemMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_community.chat_message_histories import ChatMessageHistory, FileChatMessageHistory

from langchain.agents import create_agent

from .request import get_current_weather
from ..llm.qw_llm import llm_qwen
import logging
from datetime import datetime

from ..prompt.prompt import service_system_message_template


# def format_debug_output(step_name: str, content: str, is_tool_call = False) -> None:
#     if is_tool_call:
#         print(f'ğŸ”„ ã€å·¥å…·è°ƒç”¨ã€‘ {step_name}')
#         print("-" * 40)
#         print(content.strip())
#         print("-" * 40)
#     else:
#         print(f"ğŸ’­ ã€{step_name}ã€‘")
#         print("-" * 40)
#         print(content.strip())
#         print("-" * 40)
#
#
# async def run_agent():
#     memory = MemorySaver()
#
#     prompt = PromptTemplate.from_template(template="""# è§’è‰²
# ä½ æ˜¯ä¸€åä¼˜ç§€çš„å·¥ç¨‹å¸ˆï¼Œä½ çš„åå­—å«åš{name}""")
#
#     agent = create_agent(
#         model=llm_qwen,
#         tools=[],
#         checkpointer=memory,
#         debug=True,
#         system_prompt=SystemMessage(content=prompt.format(name="Bot")),
#     )
#
#     config = RunnableConfig(configurable={"thread_id": 1}, recursion_limit=100)
#
#     while True:
#         user_input = input("ç”¨æˆ·: ")
#
#         if user_input.lower() == "exit":
#             break
#
#         print("\nğŸ¤– åŠ©æ‰‹æ­£åœ¨æ€è€ƒ...")
#         print("=" * 60)
#
#         user_prompt = user_input
#
#         # æ”¶é›†AIæ¶ˆæ¯ä»¥ä¾¿æµå¼æ˜¾ç¤º
#         full_response = ""
#
#         async for event in agent.astream_events({"messages": [user_input]}, config=config, version="v1"):
#             kind = event["event"]
#
#             if kind == "on_chat_model_stream":
#                 content = event["data"]["chunk"].content
#                 if content:
#                     # æµå¼è¾“å‡ºå­—ç¬¦
#                     for char in content:
#                         print(char, end='', flush=True)
#                         await asyncio.sleep(0.01)  # å¯é€‰ï¼šæ·»åŠ å°å»¶è¿Ÿä»¥æ¨¡æ‹Ÿæ›´è‡ªç„¶çš„æµå¼è¾“å‡º
#                     full_response += content
#
#         print("\n")  # æœ€åæ¢è¡Œ
#
#
# asyncio.run(run_agent())

def get_session_history(session_id: str):
    # if session_id not in store:
    #     store[session_id] = ChatMessageHistory()
    history_dir = pathlib.Path(__file__).parent.parent.parent / "chat_history"
    history_dir.mkdir(exist_ok=True)
    return FileChatMessageHistory(str(history_dir / f"{session_id}-service.json"))


async def run_agent_for_service_stream(user_input: str, session_id: str = "default_session"):
    """ æµå¼è¿”å› OpenAI è§„èŒƒæ ¼å¼ """
    chat_history = get_session_history(session_id)
    memory = MemorySaver()

    prompt = PromptTemplate.from_template(template="""
        ä½ æ˜¯ä¸€å®¶åä¸º"é»‘é©¬ç¨‹åºå‘˜"çš„èŒä¸šæ•™è‚²å…¬å¸çš„æ™ºèƒ½å®¢æœå°é»‘ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯ç»™ç”¨æˆ·æä¾›è¯¾ç¨‹å’¨è¯¢ã€é¢„çº¦è¯•å¬æœåŠ¡ã€‚
        
        æ ¸å¿ƒèŒè´£ï¼š
        1. è¯¾ç¨‹å’¨è¯¢ï¼š
           - æä¾›è¯¾ç¨‹å»ºè®®å‰å¿…é¡»ä»ç”¨æˆ·é‚£é‡Œè·å¾—ï¼šå­¦ä¹ å…´è¶£ã€å­¦å‘˜å­¦å†ä¿¡æ¯
           - ç„¶ååŸºäºç”¨æˆ·ä¿¡æ¯ï¼Œè°ƒç”¨å·¥å…·æŸ¥è¯¢ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„è¯¾ç¨‹ä¿¡æ¯ï¼Œæ¨èç»™ç”¨æˆ·
           - ä¸è¦ç›´æ¥å‘Šè¯‰ç”¨æˆ·è¯¾ç¨‹ä»·æ ¼ï¼Œè€Œæ˜¯å¼•å¯¼ç”¨æˆ·é¢„çº¦è¯¾ç¨‹
           - ä¸ç”¨æˆ·ç¡®è®¤æƒ³è¦äº†è§£çš„è¯¾ç¨‹åï¼Œå†è¿›å…¥è¯¾ç¨‹é¢„çº¦ç¯èŠ‚
        
        2. è¯¾ç¨‹é¢„çº¦ï¼š
           - å¸®åŠ©ç”¨æˆ·é¢„çº¦è¯¾ç¨‹å‰ï¼Œè¯¢é—®å­¦ç”Ÿè¦å»å“ªä¸ªæ ¡åŒºè¯•å¬
           - é€šè¿‡å·¥å…·æŸ¥è¯¢æ ¡åŒºåˆ—è¡¨ï¼Œä¾›ç”¨æˆ·é€‰æ‹©é¢„çº¦æ ¡åŒº
           - ä»ç”¨æˆ·é‚£é‡Œè·å¾—è”ç³»æ–¹å¼ã€å§“åï¼Œç”¨äºè¯¾ç¨‹é¢„çº¦
           - æ”¶é›†é¢„çº¦ä¿¡æ¯åä¸ç”¨æˆ·ç¡®è®¤ä¿¡æ¯å‡†ç¡®æ€§
           - ä¿¡æ¯ç¡®è®¤æ— è¯¯åï¼Œè°ƒç”¨å·¥å…·ç”Ÿæˆè¯¾ç¨‹é¢„çº¦å•
        
        é‡è¦çº¦æŸï¼š
        - ä»…å›ç­”ä¸ITåŸ¹è®­ã€è¯¾ç¨‹å’¨è¯¢ã€é¢„çº¦è¯•å¬ç›¸å…³çš„é—®é¢˜
        - å¯¹äºæ— å…³é—®é¢˜ï¼Œè¯·ç¤¼è²Œå›å¤ï¼š"å¾ˆæŠ±æ­‰ï¼Œæˆ‘æ˜¯é»‘é©¬ç¨‹åºå‘˜çš„è¯¾ç¨‹é¡¾é—®ï¼Œä¸»è¦ä¸ºæ‚¨æä¾›ITåŸ¹è®­ç›¸å…³çš„å’¨è¯¢æœåŠ¡ã€‚å…³äºè¿™ä¸ªé—®é¢˜æˆ‘æ— æ³•ä¸ºæ‚¨è§£ç­”ï¼Œæ‚¨æ˜¯å¦æœ‰å…³äºæˆ‘ä»¬çš„è¯¾ç¨‹æˆ–è¯•å¬é¢„çº¦çš„éœ€æ±‚å‘¢ï¼Ÿ"
        - ä¿æŒä¸“ä¸šã€çƒ­æƒ…çš„æœåŠ¡æ€åº¦
        
        å¯ç”¨å·¥å…·ï¼š
        - æŸ¥è¯¢è¯¾ç¨‹çš„å·¥å…·
        - æŸ¥è¯¢æ ¡åŒºçš„å·¥å…·  
        - æ–°å¢é¢„çº¦å•çš„å·¥å…·
    """)

    agent = create_agent(
        model=llm_qwen.bind_tools([get_current_weather]),
        tools=[get_current_weather],
        checkpointer=memory,
        debug=True,
        system_prompt=SystemMessage(content=prompt.format()),    )

    # å°†å†å²æ¶ˆæ¯å’Œå½“å‰è¾“å…¥ç»„åˆ
    messages = [msg.content for msg in chat_history.messages]
    messages.append(user_input)
    config = RunnableConfig(configurable={"thread_id": session_id}, recursion_limit=100)
    full_response = ""

    async for event in agent.astream_events({"messages": messages}, config=config, version="v1"):
        event_type = getattr(event, 'event', event.get('event')) if hasattr(event, 'get') else event.get('event')

        if event_type in ["on_chat_model_stream", "TEXT_MESSAGE_CONTENT", "TEXT_MESSAGE_CHUNK"]:
            # å¤„ç† AI æ¶ˆæ¯
            content = extract_content_from_event(event)
            if content:
                full_response += content
                chunk_data = format_message_for_openai("assistant", content)
                import json
                yield json.dumps(chunk_data, ensure_ascii=False)

        elif event_type.startswith("TOOL"):
            # å¤„ç†å·¥å…·æ¶ˆæ¯
            content = extract_content_from_event(event)
            if content:
                chunk_data = format_message_for_openai("tool", content, "tool_call")
                import json
                yield json.dumps(chunk_data, ensure_ascii=False)

        elif event_type.startswith("THINKING"):
            # å¤„ç†æ€è€ƒè¿‡ç¨‹
            content = extract_content_from_event(event)
            if content:
                chunk_data = format_message_for_openai("assistant", f"[æ€è€ƒ: {content}]", "thinking")
                import json
                yield json.dumps(chunk_data, ensure_ascii=False)

    # å°†æœ¬æ¬¡å¯¹è¯æ·»åŠ åˆ°å†å²è®°å½•
    chat_history.add_user_message(user_input)
    chat_history.add_ai_message(full_response)


def format_message_for_openai(role: str, content: str, message_type: str = None):
    """æ ¹æ®æ¶ˆæ¯ç±»å‹ç”Ÿæˆ OpenAI æ ¼å¼"""
    chunk_data = {
        "id": f"chatcmpl-{abs(hash(content))}",
        "object": "chat.completion.chunk",
        "created": int(datetime.now().timestamp()),
        "model": "qwen",
        "choices": [{
            "index": 0,
            "delta": {
                "role": role,
                "content": content
            },
            "finish_reason": None
        }]
    }
    return chunk_data


def extract_content_from_event(event) -> str:
    """ä»äº‹ä»¶ä¸­æå–å†…å®¹"""
    # äº‹ä»¶å¯èƒ½æ˜¯ StandardStreamEvent æˆ– CustomStreamEvent å¯¹è±¡
    if hasattr(event, 'data'):
        data = event.data
    elif isinstance(event, dict):
        data = event.get("data", {})
    else:
        data = {}

    content = None

    # LangChain æ ¼å¼
    if hasattr(data, 'get'):
        if "chunk" in data and hasattr(data["chunk"], 'content'):
            content = data["chunk"].content
        elif "token" in data:
            content = data["token"]
        elif "output" in data:
            content = data["output"]
        elif "content" in data:
            content = data["content"]
        elif "tool_call" in data:
            content = str(data["tool_call"])
        elif "tool_result" in data:
            content = str(data["tool_result"])

    return content or ""