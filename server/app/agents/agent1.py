import asyncio
import pathlib

from langchain_core.messages import AIMessage, ToolMessage, SystemMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_community.chat_message_histories import ChatMessageHistory, FileChatMessageHistory

from langchain.agents import create_agent

from .request import get_current_weather
from ..llm.qw_llm import llm_qwen
import logging
from datetime import datetime



# def format_debug_output(step_name: str, content: str, is_tool_call = False) -> None:
#     if is_tool_call:
#         print(f'ğŸ”„ ã€å·¥å…·è°ƒç”¨ã€‘ {step_name}')
#         print("-" * 40)
#         print(content.strip())
#         print("-" * 40)
#     else:
#         print(f"ğŸ’­ ã€{step_name}ã€‘")
#         print("-" * 40)
#         print(content.strip())
#         print("-" * 40)
#
#
# async def run_agent():
#     memory = MemorySaver()
#
#     prompt = PromptTemplate.from_template(template="""# è§’è‰²
# ä½ æ˜¯ä¸€åä¼˜ç§€çš„å·¥ç¨‹å¸ˆï¼Œä½ çš„åå­—å«åš{name}""")
#
#     agent = create_agent(
#         model=llm_qwen,
#         tools=[],
#         checkpointer=memory,
#         debug=True,
#         system_prompt=SystemMessage(content=prompt.format(name="Bot")),
#     )
#
#     config = RunnableConfig(configurable={"thread_id": 1}, recursion_limit=100)
#
#     while True:
#         user_input = input("ç”¨æˆ·: ")
#
#         if user_input.lower() == "exit":
#             break
#
#         print("\nğŸ¤– åŠ©æ‰‹æ­£åœ¨æ€è€ƒ...")
#         print("=" * 60)
#
#         user_prompt = user_input
#
#         # æ”¶é›†AIæ¶ˆæ¯ä»¥ä¾¿æµå¼æ˜¾ç¤º
#         full_response = ""
#
#         async for event in agent.astream_events({"messages": [user_input]}, config=config, version="v1"):
#             kind = event["event"]
#
#             if kind == "on_chat_model_stream":
#                 content = event["data"]["chunk"].content
#                 if content:
#                     # æµå¼è¾“å‡ºå­—ç¬¦
#                     for char in content:
#                         print(char, end='', flush=True)
#                         await asyncio.sleep(0.01)  # å¯é€‰ï¼šæ·»åŠ å°å»¶è¿Ÿä»¥æ¨¡æ‹Ÿæ›´è‡ªç„¶çš„æµå¼è¾“å‡º
#                     full_response += content
#
#         print("\n")  # æœ€åæ¢è¡Œ
#
#
# asyncio.run(run_agent())

def get_session_history(session_id: str):
    # if session_id not in store:
    #     store[session_id] = ChatMessageHistory()
    history_dir = pathlib.Path(__file__).parent.parent.parent / "chat_history"
    history_dir.mkdir(exist_ok=True)
    return FileChatMessageHistory(str(history_dir / f"{session_id}.json"))


async def run_agent_for_web_stream(user_input: str, session_id: str = "default_session"):
    """ æµå¼è¿”å› OpenAI è§„èŒƒæ ¼å¼ """
    chat_history = get_session_history(session_id)
    memory = MemorySaver()

    prompt = PromptTemplate.from_template(template="""
    # è§’è‰²
        ä½ æ˜¯ä¸€åä¼˜ç§€çš„åŠ©ç†, èƒ½å…¨æ–¹ä½çš„ç»™äºç”¨æˆ·å¸®åŠ©, ä¸”å–„äºä½¿ç”¨è‡ªèº«å·¥å…·
    """)

    agent = create_agent(
        model=llm_qwen.bind_tools([get_current_weather]),
        tools=[get_current_weather],
        checkpointer=memory,
        debug=True,
        system_prompt=SystemMessage(content=prompt.format(name="Bot")),
    )

    # å°†å†å²æ¶ˆæ¯å’Œå½“å‰è¾“å…¥ç»„åˆ
    messages = [msg.content for msg in chat_history.messages]
    messages.append(user_input)
    config = RunnableConfig(configurable={"thread_id": session_id}, recursion_limit=100)
    full_response = ""

    async for event in agent.astream_events({"messages": messages}, config=config, version="v1"):
        event_type = getattr(event, 'event', event.get('event')) if hasattr(event, 'get') else event.get('event')

        if event_type in ["on_chat_model_stream", "TEXT_MESSAGE_CONTENT", "TEXT_MESSAGE_CHUNK"]:
            # å¤„ç† AI æ¶ˆæ¯
            content = extract_content_from_event(event)
            if content:
                full_response += content
                chunk_data = format_message_for_openai("assistant", content)
                import json
                yield json.dumps(chunk_data, ensure_ascii=False)

        elif event_type.startswith("TOOL"):
            # å¤„ç†å·¥å…·æ¶ˆæ¯
            content = extract_content_from_event(event)
            if content:
                chunk_data = format_message_for_openai("tool", content, "tool_call")
                import json
                yield json.dumps(chunk_data, ensure_ascii=False)

        elif event_type.startswith("THINKING"):
            # å¤„ç†æ€è€ƒè¿‡ç¨‹
            content = extract_content_from_event(event)
            if content:
                chunk_data = format_message_for_openai("assistant", f"[æ€è€ƒ: {content}]", "thinking")
                import json
                yield json.dumps(chunk_data, ensure_ascii=False)

    # å°†æœ¬æ¬¡å¯¹è¯æ·»åŠ åˆ°å†å²è®°å½•
    chat_history.add_user_message(user_input)
    chat_history.add_ai_message(full_response)


def format_message_for_openai(role: str, content: str, message_type: str = None):
    """æ ¹æ®æ¶ˆæ¯ç±»å‹ç”Ÿæˆ OpenAI æ ¼å¼"""
    chunk_data = {
        "id": f"chatcmpl-{abs(hash(content))}",
        "object": "chat.completion.chunk",
        "created": int(datetime.now().timestamp()),
        "model": "qwen",
        "choices": [{
            "index": 0,
            "delta": {
                "role": role,
                "content": content
            },
            "finish_reason": None
        }]
    }
    return chunk_data


def extract_content_from_event(event) -> str:
    """ä»äº‹ä»¶ä¸­æå–å†…å®¹"""
    # äº‹ä»¶å¯èƒ½æ˜¯ StandardStreamEvent æˆ– CustomStreamEvent å¯¹è±¡
    if hasattr(event, 'data'):
        data = event.data
    elif isinstance(event, dict):
        data = event.get("data", {})
    else:
        data = {}

    content = None

    # LangChain æ ¼å¼
    if hasattr(data, 'get'):
        if "chunk" in data and hasattr(data["chunk"], 'content'):
            content = data["chunk"].content
        elif "token" in data:
            content = data["token"]
        elif "output" in data:
            content = data["output"]
        elif "content" in data:
            content = data["content"]
        elif "tool_call" in data:
            content = str(data["tool_call"])
        elif "tool_result" in data:
            content = str(data["tool_result"])

    return content or ""